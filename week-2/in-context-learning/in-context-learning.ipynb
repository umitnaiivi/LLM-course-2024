{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "\n",
    "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
    "\n",
    "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
    "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
    "\n",
    "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
    "* a search engine is used to retrieve some relevant information\n",
    "* that information is then provided to the LLM as context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U pypdf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T14:51:09.674711Z",
     "start_time": "2024-11-09T14:51:08.306516Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from IPython.display import Markdown, display\n",
    "from pypdf import PdfReader\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T15:22:20.424369Z",
     "start_time": "2024-11-09T15:22:20.136591Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAvBLdI9tvOhuwlBPkbSy2x055QUmyRkiE\r\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "!echo $GEMINI_API_KEY\n",
    "print(\"GEMINI_API_KEY\" in os.environ)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T20:50:31.222722Z",
     "start_time": "2024-11-09T20:50:31.082517Z"
    }
   },
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(API_KEY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T15:18:32.378086Z",
     "start_time": "2024-11-09T15:18:32.375415Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select those papers that have been featured in Hugging Face papers."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BASE_URL = \"https://huggingface.co/papers\"\n",
    "page = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "h3s = soup.find_all(\"h3\")\n",
    "\n",
    "papers = []\n",
    "\n",
    "for h3 in h3s:\n",
    "    a = h3.find(\"a\")\n",
    "    title = a.text\n",
    "    link = a[\"href\"].replace('/papers', '')\n",
    "\n",
    "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})\n",
    "    \n",
    "papers.append({\"title\": \"aboutness\", \"url\": \"https://journals.uio.no/dhnbpub/article/download/11510/9543/41817\"})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T18:24:14.518409Z",
     "start_time": "2024-11-09T18:24:14.066610Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'aboutness',\n 'url': 'https://journals.uio.no/dhnbpub/article/download/11510/9543/41817'}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T16:09:40.000635Z",
     "start_time": "2024-11-09T16:09:39.997965Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_paper(url):\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_pdf(url):\n",
    "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
    "    reader = PdfReader(\"pdf_file.pdf\")\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T16:09:43.903794Z",
     "start_time": "2024-11-09T16:09:43.900170Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting BERT to Model Remediation Processes in\n",
      "Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Ümit Bedretdin1,∗, Pihla Toivanen1 and Eetu Mäkelä1\n",
      "1University of Helsinki\n",
      "Abstract\n",
      "This paper showcases a supervised machine learning classifier to bridge the gap between qualitative\n",
      "and quantitative research in media studies, leveraging recent advancements in data-driven approaches.\n",
      "Current machine learning methods make it possible to gain insights from large datasets that would be\n",
      "impractical to analyze with more traditional methods. Supervised document classification presents a\n",
      "good platform for combining specific domain knowledge and close reading with broader quantitative\n",
      "analysis. The study focuses on a dataset of 37 185 articles from the Finnish countermedia publication MV-\n",
      "lehti, from which a randomly sampled 997 articles were annotated into three categories based on frame\n",
      "analysis. Contextual sequence representations from the finBERT language model, topic distributions\n",
      "from a trained topic model, and a structural, HTML-aware featureset developed in prior work are\n",
      "employed as classification features. The hypothesis that BERT-based embeddings could be improved\n",
      "upon by augmenting them with additional information is supported by recent promising results in\n",
      "natural language benchmarks and tasks (Peinelt, Nguyen, and Liakata2020; Glazkova2021). In our study,\n",
      "combining contextual embeddings with topics resulted in only marginal performance increases, and\n",
      "this improvement was observed mostly in minority classes. Despite this, potential future developments\n",
      "to achieve better classification performance are outlined. Based on the experiments, automated frame\n",
      "analysis with neural classifiers is possible, but the accuracy is not yet sufficient for inferences of high\n",
      "certainty.\n",
      "Keywords\n",
      "computational media studies, language technology, classification algorithms\n",
      "1. Introduction\n",
      "A rise in populist political style, characterised by recent discourses on concepts such as “the post\n",
      "truth era” and ”fake news” is part of an ongoing renegotiation of epistemic hierarchies in the\n",
      "contextofWesternliberaldemocracies. InFinland, thisdevelopmenthasspawnedcountermedia\n",
      "publications that position themselves against the hegemonic mainstream media, or the “elites”\n",
      "Digital Humanities in the Nordic and Baltic Countries’24, May 27–31, 2024, Reykjavík, Iceland\n",
      "∗Corresponding author.\n",
      "Envelope-Open umit.bedretdin@helsinki.fi (Ü. Bedretdin);pihla.toivanen@helsinki.fi (P. Toivanen);eetu.mäkelä@helsinki.fi\n",
      "(E. Mäkelä)\n",
      "GLOBE https://www.linkedin.com/in/%C3%BCmit-bedretdin/ (Ü. Bedretdin);\n",
      "https://researchportal.helsinki.fi/en/persons/pihla-toivanen (P. Toivanen);\n",
      "https://researchportal.helsinki.fi/en/persons/jarkko-ilkka-eetu-m%C3%A4kel%C3%A4 (E. Mäkelä)\n",
      "Orcid 0009-0008-3573-4537 (Ü. Bedretdin);0000-0002-8366-8414 (E. Mäkelä)\n",
      "© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\n",
      "Digital Humanities in the Nordic and Baltic Countries Publications – ISSN: 2704-1441\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "(Valaskivi and Robertson 2022; Toivanen, Nelimarkka, and Valaskivi 2022; Waisbord 2018). These \n",
      "countermedia seek to polarize the attitudes of like-minded individuals towards topics such as \n",
      "immigration and refugees. They operate by remediating information from the mainstream news \n",
      "flow and modulating it to fit their agenda, and their polarizing language has appeared in Finnish \n",
      "parliamentary politics through the populist Finns party (Tuomola 2018; Ylä-Anttila, Bauvois, \n",
      "and Pyrhönen 2019; Reunanen 2018). The flow of information between the countermedia and \n",
      "the mainstream media in Finland is scarcely researched,but an increasingly important subject to \n",
      "study in the context of a fragmented public sphere and the current misinformation landscape.\n",
      "Automated frame analysis in the misinformation field is still mostly seen as a high-level binary \n",
      "classification issue between truthful and “fake” news (Toivanen, Nelimarkka, and Valaskivi 2022; \n",
      "Kirchner and Reuter 2020; Lazer et al. 2018). In the Finnish media landscape, the dichotomy \n",
      "is somewhat less relevant than the more subtle tactics of introducing bias through curation \n",
      "of information (Ylä-Anttila, Bauvois, and Pyrhönen 2019). While false information can have \n",
      "adverse effects to society, Waisbord (2018) argues that the multi-layered interplay of competing \n",
      "sources of information presents a bigger challenge to the stability of western democracies. In \n",
      "such circumstances, our aim is to build tools to facilitate a large-scale analysis of framing in the \n",
      "misinformation field. This requires leveraging data analysis and language modelling.\n",
      "We employ a supervised neural network using BERT-based contextual embeddings augmented \n",
      "with topics and structural information with the goal of achieving automatic detection of these \n",
      "remediation tactics. This should be a complementary addition as BERTs embeddings contain \n",
      "mainly linguistic and distributional information to the level of a few sentences, whereas topics \n",
      "and structural features allow us to introduce document-level structural information. The \n",
      "classification is evaluated through a test scenario involving the classifying of MV-lehti articles.\n",
      "We answer the following research questions:\n",
      "RQ1 Can BERT-based, multi-class document classification be improved by leveraging topic \n",
      "information and structural features?\n",
      "RQ2 How can the analytical concepts necessary for investigating framing tactics be opera-\n",
      "tionalized for computer-aided analysis?\n",
      "2. Materials and methods\n",
      "In this section, we will describe the dataset along with the tools used in feature extraction, \n",
      "language modelling and classification.\n",
      "2.1. Data\n",
      "The dataset used in training and evaluation is a 997-article random sample taken from a collection \n",
      "of 37 185 articles containing remediation. The articles are from the Finnish countermedia publi-\n",
      "cation MV-lehti, gathered from their website between 8/2014 and 3/2018 (Toivanen, Nelimarkka, \n",
      "and Valaskivi 2022). MV-lehti is the most popular and by volume the largest countermedia\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Table 1\n",
      "Distribution of the classes\n",
      "Category Number of articles Mean word count\n",
      "Media criticism 81 353\n",
      "Copies from mainstream media 770 247\n",
      "Narrative 146 547\n",
      "publication in Finland: in March 2018 the website had over 800 000 visitors and was estimated\n",
      "to reach 5% of the population weekly (Tuomola2018; Reunanen 2018).\n",
      "The data contains the titles, main bodies of text contents including html markup, links to\n",
      "the articles, release and modification times, sources used and author id. All articles contain a\n",
      "textual reference or a hyperlink to a mainstream media source. This was seen as an indicator of\n",
      "at least some degree of remediation being present. A qualitative frame analysis by close reading\n",
      "of 997 articles yielded three main types of remediation illustrated in Table1. Due to the nature\n",
      "of the publication, authorship of the articles is not clear. Sources are also documented poorly,\n",
      "as an article might cite multiple sources, but only disclose one of them. The data was annotated\n",
      "by researchers in the field in Toivanen, Nelimarkka, and Valaskivi (2022), where they calculated\n",
      "a Cohens κ of 0.59.\n",
      "The media criticism class contains recycled articles from other media with added editorial\n",
      "commentary and novel use of typographical effects such as bolding, exclamations and quotes.\n",
      "Some have their titles changed from the originals. The articles contain criticism of particular\n",
      "journalists, media outlets and their editorial decisions. The narrative class consists of texts\n",
      "that use external sources to reinforce a narrative that is contrasting in its nature compared to\n",
      "the source article content. For example, a news report about the Finnish government adding\n",
      "resources to handle an increased amount of paperwork related to immigration is used to support\n",
      "anarticlethatclaimsallthemoneyisgoingtothe”pocketsoflawyers”. Copiesfrommainstream\n",
      "media are story-level copies, with mostly content copied from external sources, often rewritten\n",
      "titles and added commentary to align with the publications agenda. Examples of the classes are\n",
      "shown below:\n",
      "Media criticism:\n",
      "”Yle, Helsingin Sanomat and the Finnish christian media outlets have not reported on the\n",
      "incident which was covered by the German quality publication Welt and MV-lehti.”\n",
      "”Olli Juntunen has written out what many others have been wondering regarding the state\n",
      "of the Finnish press.”\n",
      "Copies from mainstream media:\n",
      "Title: “Good luck! On top of a new turncoat government, Finland also gets a new reality\n",
      "show couple!”\n",
      "A copy of a tabloid article about a woman opening up about domestic violence. The editors\n",
      "have added commentary to the text: “These kind of women are dangerous. Always sharing\n",
      "private matters to the police and seeking compassion. MV-lehti warns!!!!”\n",
      "Title: “Practical joke in Malminkartano - multiculturalgaystairs painted white”\n",
      "Main text about a rainbow-colored stairwell that was painted white, paraphrased from a local\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "news outlet. MV-lehti has added bolding to emphasize particular words like “rainbow-colored” \n",
      "“multiculturalism” and “pride”. The article features a picture taken from the original article, but \n",
      "with a new caption: “Previously faggotstairs - now aryan straight stairs”.\n",
      "Narrative:\n",
      "Title: “The skilled, industrious immigrant loved by the Finnish employers”\n",
      "The article is about how immigrants get jobs easier than Finns. The article cites comments from \n",
      "a Finnish online forum to claim that women exercise their innate motherly instincts by helping \n",
      "immigrants get jobs. Job statistics from the Construction Trade Union are used to substantiate \n",
      "that Finnish workers would be available for jobs.\n",
      "Articles in the narrative class are generally longer, with more pictures, references to outside \n",
      "sources like Youtube videos, blogs and a more free-form approach compared to other classes. \n",
      "They frequently contain retouched and modified images and niche references. The texts in the \n",
      "dataset vary in style: there are different lengths of articles from reposts of videos with only \n",
      "a few sentences to long analyses and rants. Article types range from police reports through \n",
      "copies from other media outlets to blog-like texts and fictional pieces.\n",
      "2.2. BERT\n",
      "BERT is a language representation model based on the principles of distributional semantics \n",
      "that suggests an equivalence between the distributional characteristics of a language and its \n",
      "semantics. Through its pretraining objectives, BERT produces language representations called \n",
      "contextual embeddings. These embeddings are token-level language representations that encode \n",
      "contextual information by learning dependencies through a stack of multi-headed self-attention \n",
      "layers. BERTs architecture allows it to calculate multiple kinds of dependencies between the \n",
      "input and output (Devlin et al. 2019). The information captured by these mechanisms has been \n",
      "shown to approximate linguistic constructions such as word order or syntactic information \n",
      "(Rogers, Kovaleva, and Rumshisky 2020). The resulting language representations are commonly \n",
      "used in a semi- or fully supervised classification t ask. We are utilizing finBERT, which is  a \n",
      "Finnish adaptation of the BERT pretraining framework (Virtanen et al. 2019).\n",
      "2.2.1. [CLS]-embedding\n",
      "The [CLS]-embedding represents the [CLS]-token which is a special token prepended by BERT \n",
      "to all input sequences to facilitate pre-training on next sentence prediction. This special token \n",
      "is not masked during pre-training, so through self-attention it is updated and learned in all \n",
      "training instances. The [CLS]-embedding thus becomes an aggregated representation of the \n",
      "input sequence and it has been successfully used for text classification tasks such as toxic \n",
      "content detection or detection of extreme sentiments (Xiang et al. 2021; Jamil et al. 2022).\n",
      "2.3. Structural features\n",
      "To complement the document modelling scheme, an 80-dimensional structural featureset de-\n",
      "veloped by Toivanen, Nelimarkka, and Valaskivi (2022) is utilized. The featureset utilizes \n",
      "information on media relationships, positional properties of links and video embedding tags, \n",
      "numbers and span lengths of various html tags. Media relationships in this context means the\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "links to external sites found in the articles. As BERT is pretrained on chains of sub-word tokens \n",
      "of natural language text, its embeddings do not contain any information about html code, but \n",
      "the researchers found that structural and stylistic features like bolding spans, distances between \n",
      "pictures and paragraph length were beneficial for classifying the dataset with support vector \n",
      "machines and random forest classifiers.\n",
      "2.4. Topics\n",
      "Topic models are a type of generative latent variable model, where the latent variables are \n",
      "multinomial distributions of token co-occurrences that are referred to as ”topics”. Topic mod-\n",
      "els consist of a user-specified number of topics as a distribution over the vocabulary, and a \n",
      "distribution over the topics for all documents (Chang et al. 2009). In studies that utilize topic \n",
      "models, it is typically assumed that the latent space is semantically meaningful, but in this work \n",
      "the models are not used to analyze the topical organisation and semantics of the dataset, but \n",
      "rather to investigate whether topic distributions could act as a proxy for the frames described \n",
      "in Toivanen, Nelimarkka, and Valaskivi (2022). To train the topic model, we used the structural \n",
      "topic model package for R. This package has been developed for the use of social scientific \n",
      "research and has been found useful for various tasks from analysis of high court judges’ tweets \n",
      "(Curry and Fix 2019) to comparative politics (Lucas et al. 2015).\n",
      "To facilitate training, the dataset was parsed with the TurkuNLP neural pipeline (Kanerva \n",
      "et al. 2018). Lemmatized nouns were chosen as features to have a mainly lexical classification \n",
      "feature to contrast with BERTs contextual embeddings and the structural feature vectors. We \n",
      "used a K=200, reasoning that a neural network can learn to ignore irrelevant topics that might \n",
      "be produced by a higher topic count. The topic model was initialized with spectral initialization, \n",
      "which has been found to outperform previous initialization methods (Margaret E. Roberts and \n",
      "Airoldi 2016).\n",
      "2.5. Classifier\n",
      "The classifier is a multi-layer perceptron built on the BertForSequenceClassification class from \n",
      "the PyTorch Huggingface library (Wolf et al. 2020). It has a fully connected linear input layer, a \n",
      "hidden ReLU layer, a dropout layer and an output layer. This network replaces the stock linear \n",
      "classification head of the class. A  modification was made on  the forward method of  the class, \n",
      "which is responsible for pooling the [CLS]-embeddings from the last hidden layer of BERT and \n",
      "feeding them into the classifier head and calculating loss. The method was modified to  be able \n",
      "to use not only BERT-based encodings, but also any additional feature vectors that could be \n",
      "deemed useful in classification. The dimensions of the network adapt to the input size: where \n",
      "all classification features are used, the input vector is longer. The structural feature vectors have \n",
      "80 dimensions, the topic vectors 200 and the [CLS]-embeddings have 768 dimensions. In the \n",
      "case of utilizing multiple features, the classifier uses the concatenation of the feature vectors. \n",
      "The output layer is of size 3 with a softmax activation. The presented models were chosen based \n",
      "on lowest loss on the evaluation dataset. The classifier architecture can be viewed on GitHub.\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Table 2\n",
      "F1 scores\n",
      "Classifier Criticisms Copies Narrative Microaveraged\n",
      "All features 0.49 0.92 0.68 0.82\n",
      "CLS 0.46 0.90 0.63 0.8\n",
      "CLS + Structural features 0.5 0.91 0.7 0.82\n",
      "Structural features 0.06 0.83 0.54 0.72\n",
      "Topics 0.5 0.83 0.32 0.69\n",
      "Topics + CLS 0.42 0.89 0.65 0.79\n",
      "Topics + Structural features 0.54 0.87 0.47 0.76\n",
      "Table 3\n",
      "Recall\n",
      "Classifier Criticisms Copies Narrative\n",
      "All features 0.65 0.86 0.72\n",
      "CLS 0.7 0.87 0.59\n",
      "CLS + Structural features 0.61 0.85 0.78\n",
      "Structural features 0.04 0.79 0.68\n",
      "Topics 0.83 0.79 0.68\n",
      "Topics + CLS 0.65 0.83 0.69\n",
      "Topics + Structural features 0.87 0.83 0.47\n",
      "2.6. Hardware and hyperparameters\n",
      "The TurkuNLP parsing process ran on CSC:s Puhti supercomputers small 1-node partition. 200G \n",
      "of memory was allocated for the CPU. The classifiers were trained on CSC:s Puhti supercomputer, \n",
      "on a single GPU node. Each node ran two Intel Xeon “Cascade Lake” processors that have 20 \n",
      "cores each running at 2.1GHz. A single Nvidia Volta V100 GPU with 32 GBs of memory was \n",
      "utilized.\n",
      "The uncased, 12-layer, 768 hidden, 110M parameter PyTorch HuggingFace implementation \n",
      "of the finBERT-base model was used. Each classifier was fine-tuned over 400  epochs with a \n",
      "batch size of 32 and a dropout probability of 0.1. BERTs weights were frozen as recommended \n",
      "for feature-based approaches by the original authors and to counter severe overfitting (Devlin \n",
      "et al. 2019).\n",
      "Cross entropy was used to calculate loss. As the dataset is very unbalanced, class weighing \n",
      "was modified to make minority class misclassifications more costly. The data was split into 80/20 \n",
      "training and evaluation sets and all hyperparameters were identical for the classifier-feature \n",
      "combinations.\n",
      "3. Results and analysis\n",
      "The combination of [CLS]-embeddings and structural features performed best overall with an f1 \n",
      "score of 0.82, while adding topics to [CLS]-embeddings lowered the performance for all classes,\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Table 4\n",
      "Precision\n",
      "Classifier Criticisms Copies Narrative\n",
      "All features 0.39 0.94 0.65\n",
      "CLS 0.34 0.92 0.68\n",
      "CLS + Structural features 0.42 0.95 0.63\n",
      "Structural features 0.08 0.87 0.45\n",
      "Topics 0.36 0.89 0.32\n",
      "Topics + CLS 0.31 0.95 0.62\n",
      "Topics + Structural features 0.39 0.92 0.47\n",
      "as seen in Table 2. The addition of topic information to [CLS]-embeddings reduced performance \n",
      "by 4 f1 points in the media criticism class and 3 points in the narrative class. The microaveraged \n",
      "f1 score shows a 1 point drop, but the performance impact is higher when classifying minority \n",
      "classes. [CLS]-embeddings on their own performed well, with an accuracy of 0.80.\n",
      "Structural features showed a more complementary effect, as the best overall accuracy was \n",
      "achieved with a combination of structural features and [CLS]-embeddings. This combination \n",
      "yielded an f1 score of 0.82, a 2-point increase over the [CLS]-baseline. Structural features syner-\n",
      "gised with topic vectors, yielding the highest score for the media criticism class. Performance on \n",
      "the majority class was good across all classifiers, with high recall, precision and f1 scores. Topics \n",
      "and structural features had similar performance (0.72 vs. 0.69), but structural features showed \n",
      "significantly better results on the narrative class (0.54 vs 0.32), while topic-based classification \n",
      "performed better on the criticism class (0.5 vs 0.06). Across all classes, adding structural features \n",
      "to [CLS]-embeddings was more beneficial than adding topics to [CLS]-embeddings.\n",
      "The best classification performance for the media criticism class was achieved by a combina-\n",
      "tion of topic and structural information. Topics and structural features also have the highest \n",
      "recall in the class, as shown in Table 3 (0.87), but at the cost of lower precision in Table 4 (0.39). \n",
      "As a standalone feature, topics performed better than the [CLS]-embeddings.\n",
      "For the narrative class, the best classifier utilized [CLS]-embeddings and structural features. \n",
      "This classifier outperformed the [CLS] baseline by 7 points (0.7 vs. 0.63). In the narrative class, \n",
      "purely [CLS]-based classification yields the highest precision at 0.68 points. Conversely, for \n",
      "the narrative class, the highest recall (0.78) is produced by [CLS]-embeddings combined with \n",
      "structural features. Notably, structural features considerably outperformed topics in the original \n",
      "content class (0.54 vs. 0.32).\n",
      "All classifiers did well on the majority class, with topic and structural feature classifiers \n",
      "trailing behind the [CLS]-based classification. Larger performance differences were observed \n",
      "for the minority classes.\n",
      "3.1. Analysis\n",
      "The highest performing feature combination was achieved by adding structural features to \n",
      "[CLS]-embeddings. Adding topic information yielded the same accuracy. Judging by the inter-\n",
      "annotator score, we are most likely as close to the ground truth as possible, so introducing more\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "information to the classifier is not bringing about additional performance increases. Ambiguities \n",
      "in category definitions can contribute to misclassification, particularly in the grey area between \n",
      "categories.\n",
      "Notably, the criticism and the narrative classes pose challenges for classifiers, as limited \n",
      "training examples impact feature learning. The structural feature classifier struggled due to \n",
      "the structural similarity between criticisms of mainstream media and copies, emphasizing the \n",
      "neural network’s tendency to favor majority class features. Adding cost to minority class \n",
      "misclassification did not mitigate the issue. Basic topic modelling using lemmatized nouns lacks \n",
      "the nuance required for complex classification tasks centered on immigration issues. Consider \n",
      "the following translated excerpt: ”Yle, Helsingin Sanomat and the Finnish christian media outlets \n",
      "have not reported on the incident which was covered by the German quality publication Welt \n",
      "and MV-lehti.” The media criticism in this sentence is not obvious from lemmatized nouns, \n",
      "but is rather encoded in a more complex construction ”X has not reported on Y, but Z has”. \n",
      "The different types of recontextualization may well be encoded in aspects such as dependency \n",
      "structures, use of verbs or modifiers. BERT’s success may be attributed to its encoding of these \n",
      "complex dependencies (Rogers, Kovaleva, and Rumshisky 2020).\n",
      "Despite the seeming simplicity of distinguishing copied text from criticism, challenges arise \n",
      "from how criticism is often presented intertwined with commentary. Loose editorial practices \n",
      "complicate classification for both humans and neural networks. The independent narrative \n",
      "frame shares lexical and syntactic similarities with the criticism frame, further contributing to \n",
      "misclassifications. For example, criticising a politician can look similar to criticising a journalist.\n",
      "4. Conclusions\n",
      "While a 2-point performance increase was achieved, the practical benefits of this are questionable, \n",
      "considering the added overhead of both producing a topic model and developing pre-built custom \n",
      "features. In practice, as the evaluation set is quite small, the performance differences of such \n",
      "margins come from 1-2 differently classified articles. This can easily be  attributed to chance \n",
      "differences in classification. The structural featureset that was previously shown to perform well \n",
      "with a random-forest classifier was confirmed to be helpful also to an MLP classifier. Structural \n",
      "features also synergised with the deep nondirectional embeddings extracted from BERT.\n",
      "Although Peinelt, Nguyen, and Liakata (2020) and Glazkova (2021) improved their classifi-\n",
      "cation performance by adding topic information to [CLS]-embeddings, the same effect was \n",
      "not achieved in our task. The limited amount of training examples for the minority classes \n",
      "also poses a challenge for the chosen methodology. While the majority class seems to be \n",
      "learned well enough, classifying the minority classes remained difficult. This is also reflected in \n",
      "inter-annotator errors. The differences between the criticisms of mainstream media frame and \n",
      "the independent narrative frame were fuzzy, and together with a limited amount of training \n",
      "data and possibly sub-optimal hyperparameter configuration, resulted in poor performance for \n",
      "the minority classes.\n",
      "The impact of this research is related to the broader field of media studies, where supervised \n",
      "machine learning methods have not been used to study remediation tactics of the Finnish \n",
      "countermedia. Recognizing the defining features related to recontextualization and remediation\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "is crucial for understanding the mode of operation of these kinds of publications.\n",
      "There is future potential in creating custom featuresets to enhance classification performance\n",
      "on minority classes in situations with limited and unbalanced training data as shown by the\n",
      "performance increases observed for the minority classes. As mentioned earlier, the topic model\n",
      "was trained on lemmatized nouns, which likely flattens a lot of the nuance needed to capture the\n",
      "categories. Future studies could utilize verbs or other parts of speech in modelling. Considering\n",
      "the performance benefits of the structural feature vectors, there could also be potential in utiliz-\n",
      "ing a transformer model that is trained to parse html code to help in classification. Furthermore,\n",
      "there is a lot more information encoded within BERT that was unexplored. Combinations of\n",
      "different layers and special tokens could yield different results. Even still, the [CLS]-embedding\n",
      "taken from the final layer still represents a great baseline for classification because of its ease of\n",
      "use and good performance.\n",
      "Acknowledgments\n",
      "We thank the Flows of Power and RETOSTRA research groups for support and to the Academy\n",
      "of Finland and the KONE foundation for funding this research.\n",
      "References\n",
      "Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber, and David M Blei. 2009.\n",
      "“ReadingTeaLeaves:HowHumansInterpretTopicModels.” Advances in Neural Information\n",
      "Processing Systems 22. https://proceedings.neurips.cc/paper_files/paper/2009/file/\n",
      "f92586a25bb3145facd64ab20fd554ff-Paper.pdf.\n",
      "Curry, Todd A., and Michael P. Fix. 2019. “May it please the twitterverse: The use of Twitter by\n",
      "state high court judges.”Journal of Information Technology & Politics 16, no. 4 (October 2,\n",
      "2019): 379–393. issn: 1933-1681, 1933-169X, accessed July 31, 2022.https://doi.org/10.1080/\n",
      "19331681.2019.1657048. https://www.tandfonline.com/doi/full/10.1080/19331681.2019.\n",
      "1657048.\n",
      "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training\n",
      "of Deep Bidirectional Transformers for Language Understanding. Number: arXiv:1810.04805,\n",
      "arXiv:1810.04805, May 24, 2019. Accessed June 16, 2022. arXiv:1810.04805[cs]. http:\n",
      "//arxiv.org/abs/1810.04805.\n",
      "Glazkova,Anna.2021.“IdentifyingTopicsofScientificArticleswithBERT-BasedApproachesand\n",
      "TopicModeling.”In Trends and Applications in Knowledge Discovery and Data Mining, edited\n",
      "by Manish Gupta and Ganesh Ramakrishnan, 12705:98–105. Series Title: Lecture Notes\n",
      "in Computer Science. Cham: Springer International Publishing. isbn: 978-3-030-75014-5\n",
      "978-3-030-75015-2, accessed July 31, 2022.https://doi.org/10.1007/978-3-030-75015-2_10.\n",
      "https://link.springer.com/10.1007/978-3-030-75015-2_10.\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Jamil, M. Luqman, Sebastião Pais, João Cordeiro, and Gaël Dias. 2022. “Detection of extreme\n",
      "sentiments on social networks with BERT.”Social Network Analysis and Mining 12, no. 1\n",
      "(December): 55. issn: 1869-5450, 1869-5469, accessed July 31, 2022.https://doi.org/10.1007/\n",
      "s13278-022-00882-z. https://link.springer.com/10.1007/s13278-022-00882-z.\n",
      "Kanerva, Jenna, Filip Ginter, Niko Miekka, Akseli Leino, and Tapio Salakoski. 2018. “Turku\n",
      "Neural Parser Pipeline: An End-to-End System for the CoNLL 2018 Shared Task.” InPro-\n",
      "ceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal\n",
      "Dependencies, edited by Daniel Zeman and Jan Hajič, 133–142. Brussels, Belgium: Asso-\n",
      "ciation for Computational Linguistics, October.https://doi.org/10.18653/v1/K18-2013.\n",
      "https://aclanthology.org/K18-2013.\n",
      "Kirchner, Jan, and Christian Reuter. 2020. “Countering Fake News: A Comparison of Possible\n",
      "Solutions Regarding User Acceptance and Effectiveness.”Proceedings of the ACM on Human-\n",
      "Computer Interaction 4 (CSCW2 2020): 1–27. issn: 2573-0142, accessed January 12, 2024.\n",
      "https://doi.org/10.1145/3415211. https://dl.acm.org/doi/10.1145/3415211.\n",
      "Lazer, David M. J., Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill,\n",
      "Filippo Menczer, Miriam J. Metzger, et al. 2018. “The science of fake news.”Science 359, no.\n",
      "6380 (March 9, 2018): 1094–1096. issn: 0036-8075, 1095-9203, accessed January 12, 2024.\n",
      "https://doi.org/10.1126/science.aao2998. https://www.science.org/doi/10.1126/science.\n",
      "aao2998.\n",
      "Lucas, Christopher, Richard A. Nielsen, Margaret E. Roberts, Brandon M. Stewart, Alex Storer,\n",
      "and Dustin Tingley. 2015. “Computer-Assisted Text Analysis for Comparative Politics.”\n",
      "Political Analysis 23 (2): 254–277. issn: 1047-1987, 1476-4989, accessed July 31, 2022.https:\n",
      "//doi.org/10.1093/pan/mpu019. https://www.cambridge.org/core/product/identifier/\n",
      "S1047198700011736/type/journal_article.\n",
      "Margaret E. Roberts, Brandon M. Stewart, and Edoardo M. Airoldi. 2016. “A Model of Text for\n",
      "Experimentation in the Social Sciences.”Journal of the American Statistical Association\n",
      "111 (515): 988–1003.https://doi.org/10.1080/01621459.2016.1141684. eprint: https:\n",
      "//doi.org/10.1080/01621459.2016.1141684. https://doi.org/10.1080/01621459.2016.1141684.\n",
      "Peinelt, Nicole, Dong Nguyen, and Maria Liakata. 2020. “tBERT: Topic Models and BERT Joining\n",
      "Forces for Semantic Similarity Detection.” InProceedings of the 58th Annual Meeting of\n",
      "the Association for Computational Linguistics, 7047–7055. Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Linguistics. Online: Association for Compu-\n",
      "tational Linguistics. Accessed July 31, 2022.https://doi.org/10.18653/v1/2020.acl-main.630.\n",
      "https://www.aclweb.org/anthology/2020.acl-main.630.\n",
      "Reunanen, Esa. 2018.Uutismedia verkossa 2018. Reuters Institute Digital News Report. Suomen\n",
      "maaraportti.\n",
      "Rogers, Anna, Olga Kovaleva, and Anna Rumshisky. 2020. “A Primer in BERTology: What\n",
      "We Know About How BERT Works.”Transactions of the Association for Computational\n",
      "Linguistics 8 (December): 842–866. issn: 2307-387X, accessed July 31, 2022.https://doi.org/\n",
      "10.1162/tacl_a_00349. https://direct.mit.edu/tacl/article/96482.\n",
      "Augmenting BERT to Model Remediation Processes in Finnish Countermedia: Feature Comparisons for\n",
      "Supervised Text Classification\n",
      "Toivanen, Pihla, Matti Nelimarkka, and Katja Valaskivi. 2022. “Remediation in the hybrid media\n",
      "environment: Understanding countermedia in context.”New Media & Society 24:2127–2152.\n",
      "https://doi.org/10.1177/1461444821992701.\n",
      "Tuomola, Salla. 2018. “Pakolaiskeskustelu MV-lehdessä: Merkityksellistämisen mekanismit\n",
      "ideologisissa puhuttelutavoissa.”Media & viestintä 41, no. 3 (October 3, 2018). issn: 2342-\n",
      "477X, accessed July 22, 2022.https://doi.org/10.23983/mv.75324. https://journal.fi/\n",
      "mediaviestinta/article/view/75324.\n",
      "Valaskivi, Katja, and David G. Robertson. 2022. “Introduction: epistemic contestations in the\n",
      "hybrid media environment.”Popular Communication 20, no. 3 (July 3, 2022): 153–161. issn:\n",
      "1540-5702, 1540-5710, accessed October 30, 2023.https://doi.org/10.1080/15405702.2022.\n",
      "2057998. https://www.tandfonline.com/doi/full/10.1080/15405702.2022.2057998.\n",
      "Virtanen, Antti, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski,\n",
      "Filip Ginter, and Sampo Pyysalo. 2019. “Multilingual is not enough: BERT for Finnish.”\n",
      "CoRR abs/1912.07076. arXiv:1912.07076. http://arxiv.org/abs/1912.07076.\n",
      "Waisbord, Silvio. 2018. “Truth is What Happens to News: On journalism, fake news, and\n",
      "post-truth.”Journalism Studies 19, no. 13 (October 3, 2018): 1866–1878. issn: 1461-670X,\n",
      "1469-9699, accessed January 12, 2024.https://doi.org/10.1080/1461670X.2018.1492881 .\n",
      "https://www.tandfonline.com/doi/full/10.1080/1461670X.2018.1492881.\n",
      "Wolf,Thomas,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,\n",
      "Pierric Cistac, et al. 2020.HuggingFace’s Transformers: State-of-the-art Natural Language\n",
      "Processing. arXiv: 1910.03771 [cs.CL] .\n",
      "Xiang, Tong, Sean MacAvaney, Eugene Yang, and Nazli Goharian. 2021.ToxCCIn: Toxic Content\n",
      "Classification with Interpretability, arXiv:2103.01328, March 1, 2021. Accessed July 31, 2022.\n",
      "arXiv: 2103.01328[cs]. http://arxiv.org/abs/2103.01328.\n",
      "Ylä-Anttila, Tuukka, Gwenaëlle Bauvois, and Niko Pyrhönen. 2019. “Politicization of migration\n",
      "in the countermedia style: A computational and qualitative analysis of populist discourse.”\n",
      "Discourse, Context & Media 32 (December): 100326. issn: 22116958, accessed July 27, 2022.\n",
      "https://doi.org/10.1016/j.dcm.2019.100326. https://linkinghub.elsevier.com/retrieve/pii/\n",
      "S2211695819301229.\n"
     ]
    }
   ],
   "source": [
    "test = extract_pdf(\"https://journals.uio.no/dhnbpub/article/download/11510/9543/41817\")\n",
    "print(test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T16:09:19.751607Z",
     "start_time": "2024-11-09T16:09:19.286069Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:49:04.466418Z",
     "start_time": "2024-11-09T15:49:04.463849Z"
    }
   },
   "outputs": [],
   "source": [
    "LLM = \"gemini-1.5-flash\"\n",
    "model = genai.GenerativeModel(LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Gemini to summarize the papers."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:31<00:00, 10.45s/it]\n"
     ]
    }
   ],
   "source": [
    "for paper in tqdm(papers[-3:]):\n",
    "    try:\n",
    "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. Format the strenghts and weaknesses into a markup table \" + extract_pdf(paper[\"url\"])).text\n",
    "    except:\n",
    "        print(\"Generation failed\")\n",
    "        paper[\"summary\"] = \"Paper not available\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T18:25:22.872293Z",
     "start_time": "2024-11-09T18:24:51.520874Z"
    }
   },
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'aboutness',\n 'url': 'https://journals.uio.no/dhnbpub/article/download/11510/9543/41817',\n 'summary': 'This research investigates the effectiveness of a supervised machine learning classifier in automatically identifying remediation tactics in Finnish countermedia, specifically within the publication MV-lehti. The study uses BERT-based contextual embeddings, topic distributions from a trained topic model, and structural features from the articles as input for the classifier.  \\n\\n**Strengths:**\\n\\n* **Innovative approach:**  Applies machine learning to a media studies problem, bridging the gap between qualitative and quantitative research.\\n* **Large dataset:**  Utilizes a dataset of 37,185 articles, offering a comprehensive analysis of the countermedia phenomenon.\\n* **Multi-feature analysis:** Explores the effectiveness of combining various features, including BERT embeddings, topic models, and structural information.\\n\\n**Weaknesses:**\\n\\n* **Limited performance improvement:** Augmenting BERT with topics only marginally improved performance, particularly for minority classes.\\n* **Imbalanced dataset:** The dataset is heavily skewed towards the majority class, posing challenges for classifying minority classes.\\n* **Limited data exploration:** The study focuses primarily on combining features, but further exploration of BERT embeddings and other potential features could be beneficial.\\n\\nOverall, the research demonstrates the potential of automated frame analysis using BERT and other features, but highlights the limitations in accurately classifying subtle framing tactics, particularly for minority classes in unbalanced datasets.  Further research is needed to refine the methods and explore alternative feature combinations for improved classification accuracy. \\n'}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T16:13:10.364737Z",
     "start_time": "2024-11-09T16:13:10.362364Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the results to a html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:25:44.455762Z",
     "start_time": "2024-11-09T18:25:44.413512Z"
    }
   },
   "outputs": [],
   "source": [
    "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
    "with open(\"papers.html\", \"w\") as f:\n",
    "    f.write(page)\n",
    "for paper in papers[-3:]:\n",
    "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
    "    with open(\"papers.html\", \"a\") as f:\n",
    "        f.write(page)\n",
    "end = \"</head>  </html>\"\n",
    "with open(\"papers.html\", \"a\") as f:\n",
    "    f.write(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the results to this notebook as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:25:51.181481Z",
     "start_time": "2024-11-09T18:25:51.178332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**[SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](https://arxiv.org/pdf/2411.04989)**<br>The research article introduces SG-I2V, a novel framework for controllable image-to-video generation that leverages the knowledge present in a pre-trained image-to-video diffusion model without fine-tuning or external knowledge. SG-I2V achieves zero-shot trajectory control by aligning feature maps extracted from the self-attention layers of the diffusion model and optimizing the latent code to enforce similarity between features within bounding box trajectories. The authors also introduce a frequency-based post-processing step to enhance output quality by preserving the high-frequency noise expected by the diffusion model.\n\n## Strengths and Weaknesses:\n\n| Strength | Description |\n|---|---|\n| **Zero-shot control** |  SG-I2V does not require fine-tuning, making it computationally efficient and requiring no additional training data. |\n| **Unified control** | The framework offers unified control over object and camera motion by specifying bounding box trajectories. |\n| **Versatile control** | SG-I2V can control both rigid and non-rigid motions of various objects and also enables camera motion control. |\n| **Quantitative evaluation** | The authors comprehensively evaluate SG-I2V against supervised and adapted zero-shot baselines, demonstrating its competitive performance in visual quality and motion fidelity. |\n| **Ablation studies** | The authors perform extensive ablation studies to investigate the effect of various design choices and provide valuable insights into the framework's inner workings. |\n\n\n| Weakness | Description | Analysis |\n|---|---|---|\n| **Quality limitations** | The quality of generated videos is limited by the base video diffusion model, especially for subjects with complex motion or physical interactions. | The base model's capabilities inherently limit the output quality, emphasizing the need for improved diffusion models. |\n| **Potential artifacts** | The optimization process can lead to artifacts, although mitigated by frequency-based post-processing. |  The optimization process itself might introduce out-of-distribution latents, requiring further research on how to alter the denoising process while maintaining in-distribution latents. |\n| **Limited to image-to-video** | While the authors claim the framework could be extended to newly released models, its current scope is limited to image-to-video generation. |  The framework's applicability to different types of video generation, like text-to-video, remains to be explored. |\n| **Ethical considerations** | The authors acknowledge the potential for misuse of high-quality video generation, highlighting the need for responsible and safe use. |  The increasing realism of synthesized videos raises ethical concerns regarding potential manipulation and misinformation, emphasizing the need for robust ethical guidelines and safeguards. |\n<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**[M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models](https://arxiv.org/pdf/2411.04075)**<br>This research article introduces M3S CIQA, a multi-modal, multi-document scientific question answering benchmark for evaluating foundation models' ability to handle complex research workflows. M3S CIQA surpasses existing benchmarks by incorporating both visual and textual data, requiring models to reason across multiple documents. The benchmark contains 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, each comprised of an anchor paper and its cited documents. The authors evaluated 18 foundation models, highlighting significant performance gaps between current models and human experts, particularly in scientific image understanding and long-range information retrieval. \n\n## Strengths and Weaknesses of the Research:\n\n| Strength | Weakness | Analysis |\n|---|---|---|\n| **Introduction of a novel benchmark with multi-modality and multi-document reasoning.** This addresses a crucial gap in existing scientific QA benchmarks. | **Limited context window in open-source LMMs restricts their ability to handle full paper clusters.** This leads to \"unfair\" comparisons and impacts ranking accuracy.  | The authors acknowledge this limitation and propose future work to standardize or extend context windows in LMMs. |\n| **Comprehensive evaluation of various open-source and proprietary LLMs and LMMs.** This provides valuable insights into the current state of foundation models in scientific QA. | **Prompting LMMs with a set of possible reference papers can be suboptimal.** It presents challenges for models in ranking a large number of papers. | The authors suggest using individual paper embeddings and comparing them with the textual embedding of the question and image, as an alternative approach for future research. |\n| **Detailed analysis of model limitations in visual reasoning, paper ranking, and long-range retrieval.** This offers valuable insights for improving foundation models in these areas. | **Use of GPT-4o's textual descriptions of images for BM25 and Contriever might not accurately capture image nuances.**  | This highlights the need for specialized LMMs trained on scientific images to enhance scientific applications. |\n| **Clear explanation of the benchmark construction process and annotation guidelines.** This ensures transparency and reproducibility. | **LLM-based evaluation might not accurately reflect human evaluation due to models' confidence levels.** Models might provide tangentially relevant answers instead of \"I don't know,\" leading to inflated scores. | This limitation requires further investigation to better align LLM-based evaluation with human assessments. | \n<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**[aboutness](https://journals.uio.no/dhnbpub/article/download/11510/9543/41817)**<br>This research paper investigates the use of a supervised machine learning classifier, leveraging BERT-based contextual embeddings, topic models, and structural features, to automatically detect remediation tactics in Finnish countermedia publications. The study focuses on a dataset of 997 articles from MV-lehti, a popular Finnish countermedia publication, categorized into three classes: media criticism, copies from mainstream media, and narrative. While the combination of BERT embeddings and structural features achieved the best overall performance, the addition of topic information resulted in only marginal improvements, particularly for minority classes. \n\n| Strength | Weakness | Analysis |\n|---|---|---|\n| Utilizes a large dataset of Finnish countermedia articles, providing a unique and valuable case study | The dataset is unbalanced, with a majority class and two minority classes, impacting the model's ability to learn features for the minority classes | Unbalanced datasets can lead to biased models that favor the majority class, leading to poor performance on minority classes. |\n| Employs BERT-based embeddings, a powerful language representation model | BERT embeddings do not contain information about HTML code, limiting the model's ability to utilize structural features effectively | While BERT captures semantic and syntactic information, structural features require specific processing to be integrated effectively. |\n| Investigates the effectiveness of combining different feature types, including topic models and structural features | The chosen topic modeling approach, using lemmatized nouns, may not be sufficiently nuanced to capture complex framing tactics | Complex framing often involves intricate linguistic constructions that go beyond simple lemmatized nouns, requiring more sophisticated topic modeling techniques. |\n| Evaluates the performance of the classifier on a test set | The evaluation set is relatively small, limiting the generalizability of the results | A small evaluation set can lead to inaccurate assessments of model performance, particularly for minority classes. |\n\nThe study highlights the potential of automated frame analysis for studying remediation tactics in the misinformation field, but also emphasizes the limitations of current approaches. Future research could focus on developing custom feature sets, exploring more sophisticated topic modeling methods, and utilizing transformer models that can parse HTML code to improve classification performance, particularly for minority classes. \n<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for paper in papers[-3:]:\n",
    "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
    "                                                paper[\"url\"],\n",
    "                                                paper[\"summary\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
